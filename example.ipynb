{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Tutorial for Pipefunc Package\n",
    "\n",
    "The `pipefunc` package is a Python library that allows you to define functions as pipelines, with each function providing a single step in the pipeline. In this tutorial, we will explain how to use the package, based on an example notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "For the latest documentation, check out [the official documentation](https://pipefunc.readthedocs.io/en/latest/#what-is-this).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Building a Simple Pipeline\n",
    "\n",
    "Let's start by importing `pipefunc` and `Pipeline` from the `pipefunc` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import pipefunc, Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "We then define some functions using the `@pipefunc` decorator. The `@pipefunc` decorator turns these functions into pipeline steps. For each function, we specify an `output_name` which will be used to refer to the output of that function in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipefunc(output_name=\"c\")\n",
    "def f_c(a, b):\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"d\")\n",
    "def f_d(b, c, x=1):  # \"c\" is the output of f_c\n",
    "    return b * c\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"e\")\n",
    "def f_e(c, d, x=1):  # \"d\" is the output of f_d\n",
    "    return c * d * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "We now have three functions `f_c`, `f_d`, and `f_e`, which we can use to build a pipeline. Let's create a `Pipeline` object, passing our functions in the order we want them to execute. We can also enable debugging, profiling, and caching for the entire pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([f_c, f_d, f_e], debug=True, profile=True, cache_type=\"lru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Now, we have a pipeline that adds two numbers (function `f_c`), multiplies two numbers (function `f_d`), and again multiplies two numbers (function `f_e`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Visualizing the Pipeline\n",
    "\n",
    "You can visualize your pipeline using the `visualize()` method, and print the nodes in the graph using the `graph.nodes` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.visualize()\n",
    "print(\"Graph nodes:\", pipeline.graph.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.visualize_holoviews()\n",
    "print(\"Graph nodes:\", pipeline.graph.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Using the Pipeline\n",
    "\n",
    "To use the pipeline, we first get a handle for each function using the `func` method on the pipeline, passing the output name of the function we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_d = pipeline.func(\"d\")\n",
    "pf_e = pipeline.func(\"e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "We can now use these handles as if they were the original functions. The pipeline will automatically ensure that the functions are called in the correct order, passing the output of one function as the input to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = f_c(a=2, b=3)\n",
    "assert c == 5\n",
    "assert (\n",
    "    f_d(b=3, c=5)\n",
    "    == pf_d(a=2, b=3)  # We can call pf_d with different arguments\n",
    "    == pf_d(b=3, c=5)\n",
    "    == 15\n",
    ")\n",
    "assert pf_e(c=c, d=15, x=1) == pf_e(a=2, b=3, x=1) == pf_e(a=2, b=3, d=15, x=1) == 75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Alternatively, one can also use the `__call__` method on the pipeline, passing the output name of the function we want to call, and the arguments to that function. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(\"d\", b=3, c=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Function Argument Combinations\n",
    "\n",
    "To see all the possible combinations of arguments that can be passed to each function, you can use the `all_arg_combinations` property. This will return a dictionary, with function output names as keys and sets of argument tuples as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_args = pipeline.all_arg_combinations\n",
    "assert all_args == {\n",
    "    \"c\": {(\"a\", \"b\")},\n",
    "    \"d\": {(\"a\", \"b\", \"x\"), (\"b\", \"c\", \"x\")},\n",
    "    \"e\": {(\"a\", \"b\", \"d\", \"x\"), (\"a\", \"b\", \"x\"), (\"b\", \"c\", \"x\"), (\"c\", \"d\", \"x\")},\n",
    "}\n",
    "# We can get arguments for a specific function\n",
    "assert pipeline.root_args(\"e\") == (\"a\", \"b\", \"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Using the call_full_output Method\n",
    "\n",
    "The `call_full_output()` method can be used to call the function and get all the outputs from the pipeline as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_e = pipeline.func(\"e\")\n",
    "pf_e.call_full_output(a=2, b=3, x=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Direct Calling with Root Arguments (as positional arguments)\n",
    "\n",
    "You can directly call the functions in the pipeline with the root arguments using the `call_with_root_args()` method. It automatically executes all the dependencies of the function in the pipeline with the given root arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_e = pipeline.func(\"e\")\n",
    "pf_e.call_with_root_args(1, 2, 1)  # note these are now positional args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "This executes the function `f_e` with the root arguments `a=1, b=2, x=1`.\n",
    "\n",
    "For more information about this method, you can use the Python built-in `help` function or the `?` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pf_e.call_with_root_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "This shows the signature and the doc-string of the `call_with_root_args` method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Handling Multiple Outputs\n",
    "\n",
    "Functions can return multiple results at once. The `output_name` argument allows you to specify multiple outputs by providing a tuple of strings. By default, this assumes the output is a tuple. However, if the output is a single element selected from a tuple, you can use the `output_picker` argument to specify that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import pipefunc, Pipeline\n",
    "\n",
    "\n",
    "# Define a function add_ab with multiple outputs, 'c' and 'const'.\n",
    "@pipefunc(output_name=(\"c\", \"const\"))\n",
    "def add_ab(a, b):\n",
    "    return (a + b, 1)\n",
    "\n",
    "\n",
    "# Define a function mul_bc with multiple outputs, 'd' and 'e',\n",
    "# where output_picker is used to select the output.\n",
    "@pipefunc(\n",
    "    output_name=(\"d\", \"e\"),\n",
    "    output_picker=dict.__getitem__,\n",
    ")\n",
    "def mul_bc(b, c, x=1):\n",
    "    return {\"d\": b * c, \"e\": x}\n",
    "\n",
    "\n",
    "# Define a function calc_cde with multiple outputs, 'g' and 'h',\n",
    "# where output_picker is used to select the output.\n",
    "@pipefunc(\n",
    "    output_name=(\"g\", \"h\"),\n",
    "    output_picker=getattr,\n",
    ")\n",
    "def calc_cde(c, d, e, x):\n",
    "    from types import SimpleNamespace\n",
    "\n",
    "    return SimpleNamespace(g=c * d * x, h=c + e)\n",
    "\n",
    "\n",
    "# Define a function add_gh with a single output 'i'.\n",
    "@pipefunc(output_name=\"i\")\n",
    "def add_gh(h, g):\n",
    "    return h + g\n",
    "\n",
    "\n",
    "# Create a pipeline with the defined functions and visualize it.\n",
    "pipeline_multiple = Pipeline([add_ab, mul_bc, calc_cde, add_gh])\n",
    "pipeline_multiple.visualize()\n",
    "final_func = pipeline_multiple.func(\"i\")\n",
    "final_func(a=1, b=2, x=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "**(Sneak peak of the next section: simplifying the pipeline)**\n",
    "\n",
    "The pipeline can be simplified by combining `calc_cde` and `add_gh` into a single pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_pipeline_multiple = pipeline_multiple.simplified_pipeline(\"i\")\n",
    "simplified_pipeline_multiple.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Note that, in the simplified pipeline, the full output of `calc_cde` (i.e., `g, h`) is not available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# If the full output of calc_cde (g, h) is needed, we can't use the simplified pipeline.\n",
    "out_full = pipeline_multiple.func(\"i\").call_full_output(a=1, b=2, x=3)\n",
    "out_full_red = simplified_pipeline_multiple.func(\"i\").call_full_output(a=1, b=2, x=3)\n",
    "print(f\"Full output of f_e:\\n{out_full}\")\n",
    "print(f\"Full output of f_e after simplification:\\n{out_full_red}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Simplifying Pipelines\n",
    "Consider the following pipeline (look at the `visualize()` output to see the structure of the pipeline):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import Pipeline\n",
    "\n",
    "\n",
    "def f1(a, b, c, d):\n",
    "    return a + b + c + d\n",
    "\n",
    "\n",
    "def f2(a, b, e):\n",
    "    return a + b + e\n",
    "\n",
    "\n",
    "def f3(a, b, f1):\n",
    "    return a + b + f1\n",
    "\n",
    "\n",
    "def f4(f1, f3):\n",
    "    return f1 + f3\n",
    "\n",
    "\n",
    "def f5(f1, f4):\n",
    "    return f1 + f4\n",
    "\n",
    "\n",
    "def f6(b, f5):\n",
    "    return b + f5\n",
    "\n",
    "\n",
    "def f7(a, f2, f6):\n",
    "    return a + f2 + f6\n",
    "\n",
    "\n",
    "# If the functions are not decorated with @pipefunc,\n",
    "# they will be wrapped and the output_name will be the function name\n",
    "pipeline_complex = Pipeline([f1, f2, f3, f4, f5, f6, f7])\n",
    "pipeline_complex(\"f7\", a=1, b=2, c=3, d=4, e=5)\n",
    "pipeline_complex.visualize(color_combinable=True)  # combinable functions have the same color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "In the example code above, the complex pipeline composed of multiple functions (`f1`, `f2`, `f3`, `f4`, `f5`, `f6`, `f7`) can be simplified by merging the nodes `f1`, `f3`, `f4`, `f5`, `f6` into a single node.\n",
    "This merging process simplifies the pipeline and allows to reduce the number of functions that need to be cached/saved.\n",
    "\n",
    "The method `reduced_pipeline` from the `Pipeline` class is used to generate this simplified version of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_pipeline_complex = pipeline_complex.simplified_pipeline(\"f7\")\n",
    "simplified_pipeline_complex.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "However, simplifying a pipeline comes with a trade-off. The simplification process removes intermediate nodes that may be necessary for debugging or inspection.\n",
    "\n",
    "For instance, if a developer wants to monitor the output of `f3` while processing the pipeline, they would not be able to do so in the simplified pipeline as `f3` has been merged into a single node. Hence, while a simplified pipeline can speed up the computation, it may limit the ability to examine intermediate computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Another graph simplification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import pipefunc, Pipeline\n",
    "\n",
    "\n",
    "@pipefunc(output_name=(\"d\", \"e\"))\n",
    "def calc_de(b, g, x=1):\n",
    "    pass\n",
    "\n",
    "\n",
    "@pipefunc(output_name=(\"g\", \"h\"))\n",
    "def calc_gh(a, x=1):\n",
    "    pass\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"gg\")\n",
    "def calc_gg(g):\n",
    "    pass\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"i\")\n",
    "def calc_i(gg, b, e):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Create a pipeline with the defined functions and visualize it\n",
    "pipe3 = Pipeline([calc_de, calc_gh, calc_i, calc_gg])\n",
    "pipe3.visualize(color_combinable=True)\n",
    "pipe3.simplified_pipeline(\"i\").visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Working with Resources Report\n",
    "\n",
    "The `resources_report()` method of the `pipeline` provides useful information on the performance of the functions in the pipeline such as CPU usage, memory usage, average time, and the number of times each function was called. This feature is only available if `profile=True` when creating the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will print the number of times each function was called\n",
    "# CPU, memory, and time usage is also reported\n",
    "pipeline.resources_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "This report can be beneficial in performance tuning and identifying bottlenecks in your pipeline. You can identify which functions are consuming the most resources and adjust your pipeline accordingly.\n",
    "\n",
    "You can also look all the stats directly with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.profiling_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Parallel Execution and Caching\n",
    "\n",
    "To enable parallel execution, you can use Python's built-in `concurrent.futures.ProcessPoolExecutor`. To enable caching, simply set the `cache` attribute to `True` for each function. This can be useful to avoid recomputing results when calling the same function with the same arguments multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "for f in pipeline.functions:\n",
    "    # Enable caching for all functions\n",
    "    # See next section to only cache based on a certain parameter sweep\n",
    "    f.cache = True\n",
    "\n",
    "pf_e = pipeline.func(\"e\")\n",
    "with ProcessPoolExecutor(max_workers=1) as executor:\n",
    "    results = executor.map(pf_e.call_with_dict, [{\"a\": 2, \"b\": 3, \"x\": 1}] * 10)\n",
    "    print(list(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "The cache is populated *__even when using parallel execution__*. To see the cache, you can use the `cache` attribute on the pipeline.\n",
    "\n",
    "The keys of the cache are always in terms of the root arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(f\"Cache object: {pipeline.cache}\")\n",
    "pipeline.cache.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## Parameter Sweeps\n",
    "\n",
    "Parameter sweeps are a technique used in computational simulations to explore the parameter space of a model or system. \n",
    "\n",
    "In the provided example, the `generate_sweep` method is used to generate a set of combinations of input parameters `a`, `b`, `c`, `d`, and `e` for the function `f7`. \n",
    "The `generate_sweep` method takes a dictionary of parameters as input and returns a list of dictionaries, where each dictionary represents a combination of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import Sweep\n",
    "\n",
    "combos = {\n",
    "    \"a\": [0, 1, 2],\n",
    "    \"b\": [0, 1, 2],\n",
    "    \"c\": [0, 1, 2],\n",
    "    \"d\": [0, 1, 2],\n",
    "    \"e\": [0, 1, 2],\n",
    "}\n",
    "# This means a Cartesian product of all the values in the lists\n",
    "# while zipping (\"a\", \"b\").\n",
    "sweep = Sweep(combos, dims=[(\"a\", \"b\"), \"c\", \"d\", \"e\"])\n",
    "sweep.list()[:10]  # show the first 10 combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "The function `set_cache_for_sweep` then enables caching for nodes in the pipeline that are expected to be executed two or more times during the parameter sweep. Caching improves the efficiency of the sweep by storing and reusing results of repeated computations, rather than performing the same computation multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import set_cache_for_sweep\n",
    "\n",
    "set_cache_for_sweep(\n",
    "    \"f7\", simplified_pipeline_complex, sweep, min_executions=2, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Large parameter sweeps can be computationally expensive. Reducing the pipeline and utilizing caching, as demonstrated in the example, can help alleviate this cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "## Calculating Optimal Execution Order\n",
    "\n",
    "In complex pipelines, especially those involving parameter sweeps, some function nodes may be executed multiple times. \n",
    "Precalculating and caching the results of such functions can significantly speed up the pipeline execution.\n",
    "The `pipefunc` package provides the `get_precalculation_order()` function to determine the optimal execution order of functions in a pipeline, prioritizing those functions which are executed more often.\n",
    "\n",
    "Let's consider a test scenario with a pipeline composed of four functions and a sweep of parameters `x`, `y`, and `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import pipefunc, Pipeline, Sweep, count_sweep, get_precalculation_order\n",
    "\n",
    "\n",
    "@pipefunc(output_name=(\"a\", \"b\"))\n",
    "def f_ab(y, z=1):\n",
    "    print(f\"🏃 Running f_ab(y={y}, z={z})\")\n",
    "    return y + z, y * z\n",
    "\n",
    "\n",
    "@pipefunc(output_name=(\"c\", \"d\"))\n",
    "def f_cd(x, a, z=1):\n",
    "    return x + a + z, x * a * z\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"aa\")\n",
    "def f_aa(a):\n",
    "    print(f\"🏃 Running f_aa(a={a})\")\n",
    "    return a + 1\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"i\")\n",
    "def f_i(aa, x, d):\n",
    "    return aa + x + d\n",
    "\n",
    "\n",
    "pipeline_order = Pipeline([f_cd, f_ab, f_i, f_aa])\n",
    "pipeline_order.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep = Sweep({\"y\": [1, 2], \"x\": [3, 4], \"z\": [5, 6]})\n",
    "cnt = count_sweep(\"i\", sweep, pipeline_order)\n",
    "print(\"Number of executions (keys are based on root_args):\")\n",
    "cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "To determine the optimal order of execution for functions in the pipeline, we call the `get_precalculation_order()` function, passing in our pipeline and the counts of function executions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "precalculation_order = get_precalculation_order(pipeline_order, cnt)\n",
    "precalculation_order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "In the `get_precalculation_order()` function, the order is determined by the topological dependencies of the functions and the count of their executions in the context of a parameter sweep. Only functions that are executed multiple times are included in the precalculation order. This ensures that the computation is most efficient where it matters the most.\n",
    "\n",
    "In this test scenario, the order of precalculation is `[f_e, f_gg]`, meaning `f_e` and `f_gg` should be precalculated and cached before executing the rest of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of the sweep is {len(sweep)}\")\n",
    "for f in precalculation_order:\n",
    "    f.cache = True  # enable caching\n",
    "    func = pipeline_order.func(f.output_name)\n",
    "    input_args = pipeline_order.root_args(f.output_name)\n",
    "    sub_sweep = sweep.filtered_sweep(input_args)\n",
    "    print(f\"- Function `{f}` has {len(sub_sweep)} combinations\")\n",
    "    for combo in sub_sweep.generate():  # sweep as generator\n",
    "        _ = func.call_with_dict(combo)  # We just populate the cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "Now we populated the cache, however, one might also run these calculations on a cluster and save the results to disk.\n",
    "\n",
    "Note that when we are executing the pipeline to get `'i'`, we are not executing `f_ab` and `f_aa` again, but rather loading the results from the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note no print statements are shown from `f_ab` and `f_aa`\n",
    "import pandas as pd\n",
    "\n",
    "F = pipeline_order.func(\"i\")\n",
    "results = [F.call_full_output(**combo) for combo in sweep.generate()]\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "# Running \"Fan-out\" / Map-Reduce Pipelines using `MapSpec`\n",
    "Absolutely, the `mapspec` property plays a crucial role in defining how data is processed in a distributed or parallel computing context within PipeFunc. Let's emphasize and explain this concept more clearly in the documentation.\n",
    "\n",
    "---\n",
    "\n",
    "## Running \"Fan-out\" / Map-Reduce Pipelines with PipeFunc\n",
    "\n",
    "This section illustrates how to effectively use PipeFunc for \"Fan-out\" and Map-Reduce operations, highlighting the use of the `mapspec` attribute to automate and optimize the distribution of data across operations.\n",
    "\n",
    "### Example: Doubling and Summing Integers\n",
    "\n",
    "The script below demonstrates a two-step pipeline: doubling each integer in an input list, followed by summing all the doubled values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc.map import run, load_outputs\n",
    "import numpy as np\n",
    "from typing import Any\n",
    "\n",
    "@pipefunc(output_name=\"y\", mapspec=\"x[i] -> y[i]\")\n",
    "def double_it(x: int) -> int:\n",
    "    assert isinstance(x, int)\n",
    "    return 2 * x\n",
    "\n",
    "@pipefunc(output_name=\"sum\")\n",
    "def take_sum(y: np.ndarray[Any, np.dtype[np.int_]]) -> int:\n",
    "    assert isinstance(y, np.ndarray)\n",
    "    return sum(y)\n",
    "\n",
    "pipeline = Pipeline([double_it, take_sum])\n",
    "\n",
    "inputs = {\"x\": [0, 1, 2, 3]}\n",
    "run_folder = \"my_run_folder\"\n",
    "results = run(pipeline, inputs, run_folder=run_folder)\n",
    "assert results[-1].output == 12\n",
    "assert results[-1].output_name == \"sum\"\n",
    "assert load_outputs(\"sum\", run_folder=run_folder) == 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Key Components\n",
    "\n",
    "1. **Using `mapspec` for Data Distribution**:\n",
    "   - The `mapspec` attribute in the `@pipefunc` decorator defines how data is distributed across computations. In `double_it`, `mapspec=\"x[i] -> y[i]\"` specifies that each element `i` of the input array `x` is independently processed to produce the corresponding element `i` in the output array `y`. Because `take_sum` does not have a `mapspec`, it receives the entire array `y` for aggregation.\n",
    "   - This specification allows PipeFunc to automatically handle the distribution of input elements across multiple processing units, crucial for scaling operations across large datasets or distributed environments.\n",
    "\n",
    "2. **Function Definitions**:\n",
    "   - `double_it`: Doubles each integer, demonstrating a simple stateless operation that can be easily parallelized.\n",
    "   - `take_sum`: Aggregates all elements of the resulting array into a single sum, serving as the reduce step in this Map-Reduce example.\n",
    "\n",
    "3. **Pipeline Execution**:\n",
    "   - The `run` function executes the pipeline with specific inputs and a directory for temporary run files, showcasing how PipeFunc manages data flow and execution state across the pipeline. Alternatively, use `Pipeline.map` to run the pipeline on a single input dictionary.\n",
    "\n",
    "4. **Results Verification**:\n",
    "   - Assertions check that the final sum of doubled numbers is correct, ensuring both the integrity and correctness of the pipeline's execution.\n",
    "\n",
    "5. **Output Retrieval**:\n",
    "   - The `load_outputs` function demonstrates how to retrieve and verify results post-computation, confirming the output is stored and accessible as expected.\n",
    "\n",
    "This detailed example not only demonstrates the use of basic PipeFunc functionalities but specifically underscores how the `mapspec` attribute optimizes data processing tasks by defining clear mapping rules for data elements, pivotal for implementing efficient parallel processing workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
